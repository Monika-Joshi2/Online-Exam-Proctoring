{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eea04f-11c0-4eda-8f76-3f72af06e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "10\n",
      "14\n",
      "14\n",
      "6\n",
      "20\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "14\n",
      "15\n",
      "0\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame and convert it to grayscale\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Define thresholds for motion detection\n",
    "MOUTH_MIN_THRESHOLD = 0.8  # Adjust for mouth motion sensitivity\n",
    "MOUTH_MAX_THRESHOLD = 1\n",
    "FACE_STILL_THRESHOLD = 0.8  # Threshold to determine if the face is still\n",
    "\n",
    "mou_count=0\n",
    "f_count=0\n",
    "m_count=0\n",
    "f_flag=0\n",
    "m_flag=0\n",
    "mou_flag=0\n",
    "txt_dur=10\n",
    "\n",
    "font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontscale=1\n",
    "thickness=1\n",
    "color=(0,0,255)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    gray_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        # Define the face region of interest (ROI)\n",
    "        face_roi_prev = prev_gray[y:y + h, x:x + w]\n",
    "        face_roi_next = gray_frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Calculate optical flow for the face region\n",
    "        flow = cv2.calcOpticalFlowFarneback(face_roi_prev, face_roi_next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Compute magnitude and angle of the flow for the face region\n",
    "        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        avg_face_motion = np.mean(magnitude)\n",
    "\n",
    "        # Check if the face is still\n",
    "        face_is_still = avg_face_motion < FACE_STILL_THRESHOLD\n",
    "\n",
    "        if face_is_still:\n",
    "            # Define the mouth region (lower third of the face bounding box)\n",
    "            mouth_y1 = y + int(2 * h / 3)\n",
    "            mouth_y2 = y + h\n",
    "            mouth_x1 = x\n",
    "            mouth_x2 = x + w\n",
    "\n",
    "            # Extract the mouth region of interest (ROI)\n",
    "            mouth_roi_prev = prev_gray[mouth_y1:mouth_y2, mouth_x1:mouth_x2]\n",
    "            mouth_roi_next = gray_frame[mouth_y1:mouth_y2, mouth_x1:mouth_x2]\n",
    "\n",
    "            # Calculate optical flow for the mouth region\n",
    "            mouth_flow = cv2.calcOpticalFlowFarneback(mouth_roi_prev, mouth_roi_next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "            # Compute magnitude and angle of the flow for the mouth region\n",
    "            mouth_magnitude, mouth_angle = cv2.cartToPolar(mouth_flow[..., 0], mouth_flow[..., 1])\n",
    "\n",
    "            # Check for significant motion in the mouth area\n",
    "            avg_mouth_motion = np.mean(mouth_magnitude)\n",
    "            if MOUTH_MIN_THRESHOLD < avg_mouth_motion < MOUTH_MAX_THRESHOLD:\n",
    "                mou_count=mou_count+1\n",
    "                print(m_count)\n",
    "\n",
    "            # Visualize the mouth motion\n",
    "            hsv = np.zeros((mouth_roi_prev.shape[0], mouth_roi_prev.shape[1], 3), dtype=np.uint8)\n",
    "            hsv[..., 1] = 255\n",
    "            hsv[..., 0] = mouth_angle * 180 / np.pi / 2\n",
    "            hsv[..., 2] = cv2.normalize(mouth_magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "            mouth_flow_visual = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "            # Place the mouth motion visualization on the main frame\n",
    "            frame2[mouth_y1:mouth_y2, mouth_x1:mouth_x2] = cv2.addWeighted(\n",
    "                frame2[mouth_y1:mouth_y2, mouth_x1:mouth_x2], 0.5, mouth_flow_visual, 0.5, 0\n",
    "            )\n",
    "    if len(faces)==0:\n",
    "        f_count+=1\n",
    "    if len(faces)>1:\n",
    "        m_count+=1\n",
    "\n",
    "    if f_count>20:\n",
    "        f_flag=txt_dur\n",
    "        f_count=0\n",
    "    if m_count>20:\n",
    "        m_flag=txt_dur\n",
    "        m_count=0\n",
    "    if mou_count>1:\n",
    "        mou_flag=txt_dur\n",
    "        mou_count=0\n",
    "\n",
    "    if f_flag>0:\n",
    "        cv2.putText(frame2,\"No Faces Detected !!!\",(0,30),font,fontscale,color,thickness,cv2.LINE_AA)\n",
    "        f_flag-=1\n",
    "    if m_flag>0:\n",
    "        cv2.putText(frame2,\"Multiple Faces Detected !!!\",(0,60),font,fontscale,color,thickness,cv2.LINE_AA)\n",
    "        m_flag-=1\n",
    "    if mou_flag>0:\n",
    "        cv2.putText(frame2,\"Talking Detected !!!\",(0,90),font,fontscale,color,thickness,cv2.LINE_AA)\n",
    "        mou_flag-=1\n",
    "        \n",
    "    # Display the output frame\n",
    "    cv2.imshow('Face and Mouth Motion Detection', frame2)\n",
    "\n",
    "    # Update the previous frame\n",
    "    prev_gray = gray_frame\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00addd26-1ed6-45f7-8e21-710e6192b804",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
